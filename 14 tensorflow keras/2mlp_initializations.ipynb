{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dynamic(x, y, y_1, ax, ticks,title, colors=['b']):\n",
    "    ax.plot(x, y, 'b', label=\"Train Loss\")\n",
    "    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n",
    "    if len(x)==1:\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "    plt.yticks(ticks)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_hidden_1 = 512 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "keep_prob_input = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n",
    "# h1 =>  σ=√(2/(fan_in+fan_out+1) = 0.039  => N(0,σ) = N(0,0.039)\n",
    "# h2 =>  σ=√(2/(fan_in+fan_out+1) = 0.055  => N(0,σ) = N(0,0.055)\n",
    "# out =>  σ=√(2/(fan_in+fan_out+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "\n",
    "weights_sgd = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 # sqrt(2/(784+512)) = 0.039\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128 # sqrt(2/(512+128)) = 0.055\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n",
    "# h1 =>  σ=√(2/(fan_in+1) = 0.062  => N(0,σ) = N(0,0.062)\n",
    "# h2 =>  σ=√(2/(fan_in+1) = 0.125  => N(0,σ) = N(0,0.125)\n",
    "# out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "\n",
    "weights_relu = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.062, mean=0)),    #784x512\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.125, mean=0)), #512x128\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: input (784) - sigmoid(512) - sigmoid(128) - softmax(output 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 + AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sgd = multilayer_perceptron(x, weights_sgd, biases)\n",
    "cost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_sgd, labels = y_))\n",
    "\n",
    "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use GradientDescentOptimizer\n",
    "            _, c, w = sess.run([optimizer_sgdc, cost_sgd, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: input (784) - ReLu(512) - ReLu(128) - sigmoid(output 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron_relu(x, weights, biases):\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilayer_perceptron_relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-25f3f5e62b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilayer_perceptron_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcost_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer_relu_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer_relu_sgdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multilayer_perceptron_relu' is not defined"
     ]
    }
   ],
   "source": [
    "yrelu = multilayer_perceptron_relu(x, weights_relu, biases)\n",
    "cost_relu = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = yrelu, labels = y_))\n",
    "\n",
    "optimizer_relu_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
    "optimizer_relu_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_relu_adam, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_relu, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "        \n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n",
    "    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input-ReLu(512)-ReLu(128)-sigmoid(output) - GradientDescentOptimizer\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e99332cf1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Soft Max Cross Entropy loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            _, c, w = sess.run([optimizer_relu_sgdc, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_relu, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Input - Sigmoid(BatchNormalization(512)) - Sigmoid(BatchNormalization(128))- Sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "def multilayer_perceptron_batch(x, weights, biases):\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    batch_mean_1, batch_var_1 = tf.nn.moments(layer_1,[0])\n",
    "    \n",
    "    scale_1 = tf.Variable(tf.ones([n_hidden_1]))\n",
    "    beta_1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "    layer_1 = tf.nn.batch_normalization(layer_1, batch_mean_1, batch_var_1, beta_1, scale_1, epsilon)\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    batch_mean_2, batch_var_2 = tf.nn.moments(layer_2, [0])\n",
    "    \n",
    "    scale_2 = tf.Variable(tf.ones([n_hidden_2]))\n",
    "    beta_2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "    \n",
    "    layer_2 = tf.nn.batch_normalization(layer_2, batch_mean_2, batch_var_2, beta_2, scale_2, epsilon)\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybatch = multilayer_perceptron_batch(x, weights_sgd, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_batch = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ybatch, labels = y_))\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \n",
    "optimizer_batch_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_batch)\n",
    "optimizer_batch_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_batch)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_batch_adam, cost_batch, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_batch, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(ybatch,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use GradientDescentOptimizer\n",
    "            _, c, w = sess.run([optimizer_batch_sgdc, cost_batch, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_batch, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax,np.arange(1.5, 2.4, step=0.05), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(ybatch,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Input - ReLu(512) - Dropout - ReLu(128)- Dropout -Sigmoid(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron_dropout(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    # we are adding a drop out layer after input layers with parameter keep_prob_input\n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n",
    "    layer_2_drop = tf.nn.dropout(layer_2, keep_prob)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Sigmoid activation\n",
    "    out_layer = tf.matmul(layer_2_drop, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydrop = multilayer_perceptron_dropout(x, weights_relu, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_drop = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ydrop, labels = y_))\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \n",
    "optimizer_drop_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_drop)\n",
    "optimizer_drop_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_drop)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_drop_adam, cost_drop, weights_relu], feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_drop, feed_dict={x: mnist.test.images, y_: mnist.test.labels,  keep_prob: 1.0})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(ydrop,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0 }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
