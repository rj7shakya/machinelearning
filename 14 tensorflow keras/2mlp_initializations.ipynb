{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dynamic(x, y, y_1, ax, ticks,title, colors=['b']):\n",
    "    ax.plot(x, y, 'b', label=\"Train Loss\")\n",
    "    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n",
    "    if len(x)==1:\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "    plt.yticks(ticks)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_hidden_1 = 512 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "keep_prob_input = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n",
    "# h1 =>  σ=√(2/(fan_in+fan_out+1) = 0.039  => N(0,σ) = N(0,0.039)\n",
    "# h2 =>  σ=√(2/(fan_in+fan_out+1) = 0.055  => N(0,σ) = N(0,0.055)\n",
    "# out =>  σ=√(2/(fan_in+fan_out+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "\n",
    "weights_sgd = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 # sqrt(2/(784+512)) = 0.039\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128 # sqrt(2/(512+128)) = 0.055\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n",
    "# h1 =>  σ=√(2/(fan_in+1) = 0.062  => N(0,σ) = N(0,0.062)\n",
    "# h2 =>  σ=√(2/(fan_in+1) = 0.125  => N(0,σ) = N(0,0.125)\n",
    "# out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "\n",
    "weights_relu = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.062, mean=0)),    #784x512\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.125, mean=0)), #512x128\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: input (784) - sigmoid(512) - sigmoid(128) - softmax(output 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 + AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sgd = multilayer_perceptron(x, weights_sgd, biases)\n",
    "cost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_sgd, labels = y_))\n",
    "\n",
    "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use GradientDescentOptimizer\n",
    "            _, c, w = sess.run([optimizer_sgdc, cost_sgd, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
